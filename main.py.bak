import csv

from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score

from sklearn.naive_bayes import BernoulliNB

from feature_extraction import processData


vectorizer = DictVectorizer()


def train_svm(X, y):

    svm = SVC(C=100, gamma=0.001, kernel='rbf')
    svm.fit(X, y)
    return svm


def train_BNB(X, y):
    bnb = BernoulliNB()
    bnb.fit(X_train, y_train)
    return bnb


def create_training_data(document):

    # Create the training data class labels
    y = [d[1] for d in document]

    # Create the document corpus list
    corpus = [d[0] for d in document]

    # Create the TF-IDF vectoriser and transform the corpus

    X = vectorizer.fit_transform(corpus)
    return X, y

if __name__ == "__main__":

    # prepare documents
    docs = []
    allContent = []
    allCodes = []

    # open the coded tweets csv file
    with open('change-700.csv', 'rb') as f:
        reader = csv.reader(f, delimiter=',')
        for row in reader:
            # arrange file content in the tuple, push to documents array
            allContent.append(row[2])
            allCodes.append(row[6])

    allContent = processData(allContent)

    docs = list(zip(allContent, allCodes))

    # fit training data and split for training and testing
    X, y = create_training_data(docs)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Create and train the Support Vector Machine
    svm = train_svm(X_train, y_train)
    prediction = svm.predict(X_test)

    print "SVM Result:"
    print svm.score(X_test, y_test)
    print confusion_matrix(prediction, y_test, labels=["0", "1"])
    print "true positive:"
    print precision_score(y_test, prediction, labels=None, pos_label="0", average='binary', sample_weight=None)
    print "F measurement:"
    print f1_score(y_test, prediction, average='macro')
    print f1_score(y_test, prediction, average='micro')
    print f1_score(y_test, prediction, average='weighted')

    # #naive bayes
    # def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):
    #     class_labels = classifier.classes_
    #     feature_names = vectorizer.get_feature_names()
    #     topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]
    #     topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]
    #
    #     for coef, feat in topn_class1:
    #         print class_labels[0], coef, feat
    #
    #     print
    #
    #     for coef, feat in reversed(topn_class2):
    #         print class_labels[1], coef, feat
    #
    # bnb = train_BNB(X_train, y_train)
    # print("nb Result:")
    # print(bnb.score(X_test, y_test))
    #
    # most_informative_feature_for_binary_classification(vectorizer, bnb)

    # print allContent[:10]

