# -*- coding: utf-8 -*-

import sys
import re
import csv

allContent = []

with open('change-700.csv', 'r') as f:
    reader = csv.reader(f, delimiter=',')
    for row in reader:
        # arrange file content in the tuple, push to documents array
        allContent.append(row[2])


from url_classifier import getURLType
from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()
from nltk.stem.porter import *


featureSets = []

def countNumberOfWords(tweet):
    return len(tweet.split())

def countChars(tweet):
    return len(tweet) - tweet.count(' ')

def countNumericChars(tweet):
    return sum(c.isdigit() for c in tweet)

def countHashtag(tweet):
    hashTagCounter = 0
    for word in tweet.split():
        if word.startswith("#"):
            hashTagCounter += 1
    return hashTagCounter

def countAtUser(tweet):
    atUserCounter = 0
    for word in tweet.split():
        if word.startswith("@"):
            atUserCounter += 1
    return atUserCounter

def countUrl(tweet):
    urlCounter = 0
    for word in tweet.split():
        if word.startswith("http"):
            urlCounter += 1
    return urlCounter

def countEmoji(tweet):
    myre = re.compile(u'('
                      u'\ud83c[\udf00-\udfff]|'
                      u'\ud83d[\udc00-\ude4f\ude80-\udeff]|'
                      u'[\u2600-\u26FF\u2700-\u27BF])+',
                      re.UNICODE)

    return len(re.findall(myre, tweet))

def countHashtagPerWord(tweet):
    return countHashtag(tweet) / float(len(tweet.split()))

def countUrlPerWord(tweet):
    return countUrl(tweet) / float(len(tweet.split()))

def ifStartWithHashtag(tweet):
    return tweet[0] == "#"


def ifEndWithHashtag(tweet):
    return tweet.split()[-1][0] == "#"

def ifStartWithUrl(tweet):
    return tweet.startswith("http")

def ifEndWithUrl(tweet):
    return tweet.split()[-1].startswith("http")


def ifStartWithAtUser(tweet):
    return tweet[0] == "@"


def ifEndWithAtUser(tweet):
    return tweet.split()[-1][0] == "@"


def replaceAtUser(tweet):
    tweet = re.sub(r'[@]\S*', '@user', tweet)

    return tweet

def removeHashtag(tweet):
    tweet = re.sub(r'[#]', '', tweet)

    return tweet

def classifyURL(tweet):
    urlTypes = ""
    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)
    for url in urls:
        urlTypes = urlTypes + getURLType(url) + " "

    return urlTypes


def cleanUrl(tweet):
    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', tweet)

    return tweet

def ifHasDealFeature(tweet):
    dealSigns = ["$", "free", "check out", "deal", "%", "watch now", "now live"]

    for word in dealSigns:
        if tweet.find(word) != -1:
            return True

def processData(allTweets):

    allUrls = []
    urls = open("urls.txt", "rb")
    for line in urls:
        allUrls.append(line[:-1])


    for i, tweet in enumerate(allTweets):

        features = {}

        features["urlType"] = allUrls[i]

        if ifStartWithHashtag(tweet):
            features["startWithHashtag"] = True
        elif ifEndWithHashtag(tweet):
            features["endWithHashTag"] = True

        if ifStartWithUrl(tweet):
            features["startWithUrl"] = True
        elif ifEndWithUrl(tweet):
            features["endWithUrl"] = True

        if ifStartWithAtUser(tweet):
            features["startWithAtUser"] = True
        elif ifEndWithAtUser(tweet):
            features["endWithAtUser"] = True

        features["atUserCount"] = countAtUser(tweet)

        features["urlCount"] = countUrl(tweet)

        # features["urlType"] = classifyURL(tweet)

        features["hashtagCount"] = countHashtag(tweet)

        features["hashtagPerWord"] = countHashtagPerWord(tweet)

        features["urlPerWord"] = countUrlPerWord(tweet)

        features["numberOfWords"] = countNumberOfWords(tweet)

        features["numberOfNumericChars"] = countNumericChars(tweet)

        features["numberOfChars"] = countChars(tweet)

        if ifHasDealFeature(tweet):
            features["dealLike"] = True

        tweet = cleanUrl(tweet)

        tweet = removeHashtag(tweet)

        stemmer = PorterStemmer()

        tweet = tweet.decode("utf8")

        tweetStems = [stemmer.stem(word) for word in tweet.split()]

        tweet = " ".join(tweetStems)

        terms = tknzr.tokenize(tweet)

        for t in terms:
            if t not in features:
                features[t] = 1

        featureSets.append(features)

        print("finished")
        print(i)

    return featureSets


    #!/usr/bin/python

    # Open a file



#
#
# # urls = open("urls.txt", "wb")
# #
# # for i, tweet in enumerate(allContent):
# #     urls.write(classifyURL(tweet) + "\n")
# #     print "finished"
# #     print i
# # # Close opend file97
# # urls.close()
# #
# # file = open("urls.txt", "rb")
# #
# # allUrls = []
# #
# # for line in file:
# #     allUrls.append(line[:-1])
# #
# # print allUrls
#

testData = "#Donald #Trump rejects #White #Nationalist #Support , but not #Voters ' #Anger http://bit.ly/1J3X2Ex"
#

print(allContent[1:2][0])
print(countEmoji(allContent[1:2][0]))